{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Final Evaluation (Metrics and Prediction)\n",
        "### This code tests model on new images and saves results to final metrics.\n"
      ],
      "metadata": {
        "id": "pXyLwXSL8MNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###   5.1 Load Model and Evaluate Test Set"
      ],
      "metadata": {
        "id": "8CEYHpoV8ia2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation mode\n",
        "model_ft.eval() # Put the model in evaluation mode\n",
        "# Lists that keep the real answers and what the model predicted\n",
        "y_true = []\n",
        "y_pred = []\n",
        "print(\"Evaluating the Test set...\")\n",
        "# Disable gradients to make evaluation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloaders['test']:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model_ft(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "print(\"Evaluation complete.\")"
      ],
      "metadata": {
        "id": "2-_mwfq618Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###   5.2 Classification Report (Precision, Recall, F1-score)\n",
        "### Classification report shows these metrics."
      ],
      "metadata": {
        "id": "yTw8owW88u-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "# Classification report uses true labels and predictions\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))"
      ],
      "metadata": {
        "id": "CFQbh-SN2FR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Confusion Matrix Visualization\n",
        "### This code shows how the model predicted each class."
      ],
      "metadata": {
        "id": "J8usJyrm85mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "# Visualize the matrix\n",
        "plt.figure(figsize=(18, 16))\n",
        "# Heatmap visualization of the matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Model Prediction (Predicted Label)')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5jc4lPiV2KII"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}